@article{Moreau2015a,
abstract = {Many applications that can take advantage of accelerators are amenable to approximate execution. Past work has shown that neural acceleration is a viable way to accelerate approximate code. In light of the growing availability of on-chip field-programmable gate arrays (FPGAs), this paper explores neural acceleration on off-the-shelf programmable SoCs. We describe the design and implementation of SNNAP, a flexible FPGA-based neural accelerator for approximate programs. SNNAP is designed to work with a compiler workflow that configures the neural network's topology and weights instead of the programmable logic of the FPGA itself. This approach enables effective use of neural acceleration in commercially available devices and accelerates different applications without costly FPGA reconfigurations. No hardware expertise is required to accelerate software with SNNAP, so the effort required can be substantially lower than custom hardware design for an FPGA fabric and possibly even lower than current 'C-to-gates' high-level synthesis (HLS) tools. Our measurements on a Xilinx Zynq FPGA show that SNNAP yields a geometric mean of 3.8× speedup (as high as 38.1×) and 2.8× energy savings (as high as 28 x) with less than 10{\%} quality loss across all applications but one. We also compare SNNAP with designs generated by commercial HLS tools and show that SNNAP has similar performance overall, with better resource-normalized throughput on 4 out of 7 benchmarks.},
author = {Moreau, Thierry and Wyse, Mark and Nelson, Jacob and Sampson, Adrian and Esmaeilzadeh, Hadi and Ceze, Luis and Oskin, Mark},
doi = {10.1109/HPCA.2015.7056066},
file = {:home/tomas/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moreau et al. - 2015 - SNNAP Approximate computing on programmable SoCs via neural acceleration.pdf:pdf},
isbn = {9781479989300},
journal = {2015 IEEE 21st International Symposium on High Performance Computer Architecture, HPCA 2015},
mendeley-groups = {tesis/Use NN for Approx Comp},
pages = {603--614},
publisher = {IEEE},
title = {{SNNAP: Approximate computing on programmable SoCs via neural acceleration}},
year = {2015}
}
@article{Castro-Zunti2020,
abstract = {As Intelligent Transportation Systems applications increase in prevalence, Automatic License Plate Recognition solutions must be made continually faster and more accurate. The authors propose an embedded system for fast and accurate license plate segmentation and recognition using a modified single shot detector (SSD) with a feature extractor based on depthwise separable convolutions and linear bottlenecks. The feature extractor requires less parameters than the original SSD + VGG implementation, enabling fast inference. Tested on the Caltech Cars dataset, the proposed model achieves 96.46{\%} segmentation and 96.23{\%} recognition accuracy. Tested on the UCSD-Stills dataset, the proposed model achieves 99.79{\%} segmentation and 99.79{\%} recognition accuracy. The authors achieve a per-plate (resized to 300 × 300 px) processing time of 59 ms on an Intel Xeon CPU with 12 cores (2.60 GHz per core), 14 ms using the same CPU and OpenVINO (a neural network acceleration platform), and 66 ms using the proposed low-cost Raspberry Pi 3 and Intel Neural Compute Stick 2 with OpenVINO embedded system.},
author = {Castro-Zunti, Riel D. and Y{\'{e}}pez, Juan and Ko, Seok Bum},
doi = {10.1049/iet-its.2019.0481},
file = {:home/tomas/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Castro-Zunti, Y{\'{e}}pez, Ko - 2020 - License plate segmentation and recognition system using deep learning and OpenVINO.pdf:pdf},
issn = {1751956X},
journal = {IET Intelligent Transport Systems},
mendeley-groups = {tesis/OpenVINO-Myriad},
number = {2},
pages = {119--126},
title = {{License plate segmentation and recognition system using deep learning and OpenVINO}},
volume = {14},
year = {2020}
}
@article{Yazdanbakhsh2016,
author = {Yazdanbakhsh, A. and Mahajan, D. and Lotfi-Kamran, P. and Esmaeilzadeh, H.},
file = {:home/tomas/tesis/papers/dt.darksilicon16-camera.pdf:pdf},
journal = {IEEE Design and Test},
mendeley-groups = {tesis/Misc},
number = {special issue on Computing in the Dark Silicon Era 2016},
pages = {1--7},
title = {{AxBench: A Benchmark Suite for Approximate Computing}},
url = {http://hdl.handle.net/1853/54485},
year = {2016}
}
@article{Esmaeilzadeh2012,
abstract = {This paper describes a learning-based approach to the acceleration of approximate programs. We describe the $\backslash$emph{\{}Parrot transformation{\}}, a program transformation that selects and trains a neural network to mimic a region of imperative code. After the learning phase, the compiler replaces the original code with an invocation of a low-power accelerator called a $\backslash$emph{\{}neural processing unit{\}} (NPU). The NPU is tightly coupled to the processor pipeline to accelerate small code regions. Since neural networks produce inherently approximate results, we define a programming model that allows programmers to identify approximable code regions-code that can produce imprecise but acceptable results. Offloading approximable code regions to NPUs is faster and more energy efficient than executing the original code. For a set of diverse applications, NPU acceleration provides whole-application speedup of 2.3x and energy savings of 3.0x on average with quality loss of at most 9.6{\%}. {\textcopyright} 2012 IEEE.},
author = {Esmaeilzadeh, Hadi and Sampson, Adrian and Ceze, Luis and Burger, Doug},
doi = {10.1109/MICRO.2012.48},
file = {:home/tomas/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Esmaeilzadeh et al. - 2012 - Neural acceleration for general-purpose approximate programs.pdf:pdf},
isbn = {9780769549248},
journal = {Proceedings - 2012 IEEE/ACM 45th International Symposium on Microarchitecture, MICRO 2012},
keywords = {Accelerator,Approximate Computing,NPU,Neural Networks,Neural Processing Unit},
mendeley-groups = {tesis/Use NN for Approx Comp},
pages = {449--460},
publisher = {IEEE},
title = {{Neural acceleration for general-purpose approximate programs}},
year = {2012}
}
@article{Zhang2016,
abstract = {Neural networks (NNs) have been demonstrated to be useful in a broad range of applications such as image recognition, automatic translation and advertisement recommendation. State-of-The-Art NNs are known to be both computationally and memory intensive, due to the ever-increasing deep structure, i.e., multiple layers with massive neurons and connections (i.e., synapses). Sparse neural networks have emerged as an effective solution to reduce the amount of computation and memory required. Though existing NN accelerators are able to efficiently process dense and regular networks, they cannot benefit from the reduction of synaptic weights. In this paper, we propose a novel accelerator, Cambricon-X, to exploit the sparsity and irregularity of NN models for increased efficiency. The proposed accelerator features a PE-based architecture consisting of multiple Processing Elements (PE). An Indexing Module (IM) efficiently selects and transfers needed neurons to connected PEs with reduced bandwidth requirement, while each PE stores irregular and compressed synapses for local computation in an asynchronous fashion. With 16 PEs, our accelerator is able to achieve at most 544 GOP/s in a small form factor (6.38 mm2 and 954 mW at 65 nm). Experimental results over a number of representative sparse networks show that our accelerator achieves, on average, 7.23x speedup and 6.43x energy saving against the state-of-The-Art NN accelerator.},
author = {Zhang, Shijin and Du, Zidong and Zhang, Lei and Lan, Huiying and Liu, Shaoli and Li, Ling and Guo, Qi and Chen, Tianshi and Chen, Yunji},
doi = {10.1109/MICRO.2016.7783723},
file = {:home/tomas/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2016 - Cambricon-X An accelerator for sparse neural networks.pdf:pdf},
isbn = {9781509035083},
issn = {10724451},
journal = {Proceedings of the Annual International Symposium on Microarchitecture, MICRO},
mendeley-groups = {tesis/Neural Acc Arch},
pages = {1--12},
publisher = {IEEE},
title = {{Cambricon-X: An accelerator for sparse neural networks}},
volume = {2016-Decem},
year = {2016}
}
@article{Tian2015,
abstract = {Motivated by the inherent error-resilience of emerging recognition, mining, and synthesis (RMS) applications, approximate computing techniques such as precision scaling has been advocated for achieving energy-efficiency gains at the cost of small accuracy loss. Most existing solutions, however, focus on the approximation of on-chip computations without considering that of off-chip data accesses, whose energy consumption may contribute to a significant portion of the total energy. In this work, we propose a novel approximate memory access technique for dynamic precision scaling, namely ApproxMA. To be specific, by taking both runtime data precision constraints and error-resilient capabilities of the application into consideration, ApproxMA determines the precision of data accesses and loads scaled data from off-chip memory for computation. Experimental results with mixture model-based clustering algorithms demonstrate the efficacy of the proposed methodology.},
author = {Tian, Ye and Zhang, Qian and Wang, Ting and Yuan, Feng and Xu, Qiang},
doi = {10.1145/2742060.2743759},
file = {:home/tomas/tesis/papers/2742060.2743759.pdf:pdf},
isbn = {9781450334747},
journal = {Proceedings of the ACM Great Lakes Symposium on VLSI, GLSVLSI},
keywords = {Approximate computing,Memory access,Precision scaling},
mendeley-groups = {tesis/Use Approx Comp for NN},
pages = {337--342},
title = {{ApproxMA: Approximate memory access for dynamic precision scaling}},
volume = {20-22-May-},
year = {2015}
}
@article{Venkataramani2015,
abstract = {Neuromorphic algorithms, which are comprised of highly complex, large-scale networks of artificial neurons, are increasingly used for a variety of recognition, classification, search and vision tasks. However, their computational and energy requirements can be quite high, and hence their energy-efficient implementation is of great interest. We propose a new approach to design energy-efficient hardware implementations of large-scale neural networks (NNs) using approximate computing. Our work is motivated by the observations that (i) NNs are used in applications where less-than-perfect results are acceptable, and often inevitable, and (ii) they are highly resilient to inexactness in many (but not all) of their constituent computations. We make two key contributions. First, we propose a method to transform any given NN into an Approximate Neural Network (AxNN). This is performed by (i) adapting the backpropagation technique, which is commonly used to train these networks, to quantify the impact of approximating each neuron to the overall network quality (e.g., classification accuracy), and (ii) selectively approximating those neurons that impact network quality the least. Further, we make the key observation that training is a naturally error-healing process that can be used to mitigate the impact of approximations to neurons. Therefore, we incrementally retrain the network with the approximations in-place, reclaiming a significant portion of the quality ceded by approximations. As a second contribution, we propose a programmable and quality-configurable neuromorphic processing engine (qcNPE), which utilizes arrays of specialized processing elements that execute neuron computations with dynamically configurable accuracies and can be used to execute AxNNs from diverse applications. We evaluated the proposed approach by constructing AXNNs for 6 recognition applications (ranging in complexity from 12-47,818 neurons and 160-3,155,968 connections) and executing them on two different platforms-qcNPE implementation containing 272 processing elements in 45nm technology and a commodity Intel Xeon server. Our results demonstrate 1.14X-1.92X energy benefits for virtually no loss ({\textless} 0.5{\%}) in output quality, and even higher improvements (upto 2.3X) when some loss (upto 7.5{\%}) in output quality is acceptable.},
author = {Venkataramani, Swagath and Ranjan, Ashish and Roy, Kaushik and Raghunathan, Anand},
doi = {10.1145/2627369.2627613},
file = {:home/tomas/tesis/papers/07298218.pdf:pdf},
isbn = {9781450329750},
issn = {15334678},
journal = {Proceedings of the International Symposium on Low Power Electronics and Design},
keywords = {Approximate Computing,Energy Efficiency,Large-scale Neural Networks,Neuromorphic Systems},
mendeley-groups = {tesis/Use Approx Comp for NN},
pages = {27--32},
publisher = {ACM},
title = {{AxNN: Energy-efficient neuromorphic systems using approximate computing}},
volume = {2015-Octob},
year = {2015}
}
@article{Sen2017,
abstract = {Spiking Neural Networks (SNNs) are widely regarded as the third generation of artificial neural networks, and are expected to drive new classes of recognition, data analytics and computer vision applications. However, large-scale SNNs (e.g., of the scale of the human visual cortex) are highly compute and data intensive, requiring new approaches to improve their efficiency. Complementary to prior efforts that focus on parallel software and the design of specialized hardware, we propose AxSNN, the first effort to apply approximate computing to improve the computational efficiency of evaluating SNNs. In SNNs, the inputs and outputs of neurons are encoded as a time series of spikes. A spike at a neuron's output triggers updates to the potentials (internal states) of neurons to which it is connected. AxSNN determines spike-triggered neuron updates that can be skipped with little or no impact on output quality and selectively skips them to improve both compute and memory energy. Neurons that can be approximated are identified by utilizing various static and dynamic parameters such as the average spiking rates and current potentials of neurons, and the weights of synaptic connections. Such a neuron is placed into one of many approximation modes, wherein the neuron is sensitive only to a subset of its inputs and sends spikes only to a subset of its outputs. A controller periodically updates the approximation modes of neurons in the network to achieve energy savings with minimal loss in quality. We apply AxSNN to both hardware and software implementations of SNNs. For hardware evaluation, we designed SNNAP, a Spiking Neural Network Approximate Processor that embodies the proposed approximation strategy, and synthesized it to 45nm technology. The software implementation of AxSNN was evaluated on a 2.7 GHz Intel Xeon server with 128 GB memory. Across a suite of 6 image recognition benchmarks, AxSNN achieves 1.4-5.5x reduction in scalar operations for network evaluation, which translates to 1.2-3.62x and 1.26-3.9x improvement in hardware and software energies respectively, for no loss in application quality. Progressively higher energy savings are achieved with modest reductions in output quality.},
author = {Sen, Sanchari and Venkataramani, Swagath and Raghunathan, Anand},
doi = {10.23919/DATE.2017.7926981},
file = {:media/tomas/KINGSTON1GB/07926981.pdf:pdf},
isbn = {9783981537093},
journal = {Proceedings of the 2017 Design, Automation and Test in Europe, DATE 2017},
keywords = {Approximate computing,Approximate neural networks,Spiking Neural Networks},
mendeley-groups = {tesis/Use Approx Comp for NN},
pages = {193--198},
publisher = {EDAA},
title = {{Approximate computing for spiking neural networks}},
year = {2017}
}
@article{Nourazar2018,
abstract = {In this paper, we demonstrate the feasibility of building a memristor-based approximate accelerator to be used in cooperation with general-purpose × 86 processors. First, an integrated full system simulator is developed for simultaneous simulation of any multicrossbar architecture as an accelerator for × 86 processors, which is performed by coupling a cycle accurate Marss × 86 processor simulator with the Ngspice mixed-level/mixed-signal circuit simulator. Then, a novel mixed-signal memristor-based architecture is presented for multiplying floating-point signed complex numbers. The presented multiplier is extended for accelerating convolutional neural networks and finally, it is tightly integrated with the pipeline of a generic × 86 processor. To validate the accelerator, first it is utilized for multiplying different matrices that vary in size and distribution. Then, it is used as an accelerator for accelerating the tiny-dnn, an open-source C++ implementation of deep learning neural networks. The memristor-based accelerator provides more than 100 × speedup and energy saving for a 64 × 64 matrix-matrix multiplication, with an accuracy of 90{\%}. Using the accelerated tiny-dnn for the MNIST database classification more than 10 × speedup and energy saving along with 95.51{\%} pattern recognition accuracy is achieved.},
author = {Nourazar, Mohsen and Rashtchi, Vahid and Azarpeyvand, Ali and Merrikh-Bayat, Farshad},
doi = {10.1109/TVLSI.2018.2837908},
file = {:media/tomas/KINGSTON1GB/08374046.pdf:pdf},
issn = {10638210},
journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
keywords = {Analog accelerator,approximate computing,convolutional neural network (CNN),matrix multiplier,memristor crossbar},
mendeley-groups = {tesis/Use Approx Comp for NN},
number = {12},
pages = {2684--2695},
publisher = {IEEE},
title = {{Code Acceleration Using Memristor-Based Approximate Matrix Multiplier: Application to Convolutional Neural Networks}},
volume = {26},
year = {2018}
}
@article{Peng2018,
abstract = {Neural network based approximate computing is a universal architecture promising to gain tremendous energy-efficiency for many error resilient applications. To guarantee the approximation quality, existing works deploy two neural networks (NNs), e.g., an approximator and a predictor. The approximator provides the approximate results, while the predictor predicts whether the input data is safe to approximate with the given quality requirement. However, it is non-trivial and time-consuming to make these two neural network coordinate - -they have different optimization objectives - -by training them separately. This paper proposes a novel neural network structure - -AXNet - -to fuse two NNs to a holistic end-to-end trainable NN. Leveraging the philosophy of multi-task learning, AXNet can tremendously improve the invocation (proportion of safe-to-approximate samples) and reduce the approximation error. The training effort also decrease significantly. Experiment results show 50.7{\%} more invocation and substantial cuts of training time when compared to existing neural network based approximate computing framework.},
archivePrefix = {arXiv},
arxivId = {1807.10458},
author = {Peng, Zhenghao and Chen, Xuyang and Xu, Chengwen and Jing, Naifeng and Liang, Xiaoyao and Lu, Cewu and Jiang, Li},
doi = {10.1145/3240765.3240783},
eprint = {1807.10458},
file = {:media/tomas/KINGSTON1GB/08605388.pdf:pdf},
isbn = {9781450359504},
issn = {10923152},
journal = {IEEE/ACM International Conference on Computer-Aided Design, Digest of Technical Papers, ICCAD},
keywords = {approximate computing,end-to-end learning,multitask learning,neural network,quality control},
mendeley-groups = {tesis/Use Approx Comp for NN},
pages = {10--17},
publisher = {ACM},
title = {{AXNet: Approximate computing using an end-to-end trainable neural network}},
year = {2018}
}
@article{Wang2020,
author = {Wang, Ziwei and Trefzer, Martin A. and Bale, Simon J. and Tyrrell, Andy M.},
doi = {10.1109/recosoc48741.2019.9034956},
file = {:media/tomas/KINGSTON1GB/09034956.pdf:pdf},
isbn = {9781728147703},
mendeley-groups = {tesis/Use Approx Comp for NN},
pages = {35--42},
publisher = {IEEE},
title = {{Approximate Multiply-Accumulate Array for Convolutional Neural Networks on FPGA}},
year = {2020}
}
@article{Andriyanov2020,
author = {Andriyanov, N A},
file = {:home/tomas/tesis/papers/09166067.pdf:pdf},
isbn = {9781728160726},
keywords = {coco dataset,cpu,intel,machine learning,network,networks,neural,neural network inference,object detection,openvino,pattern recognition,ssd{\_}mobilenet,the comparison is made,the performance of neural,toolkit},
mendeley-groups = {tesis,tesis/OpenVINO-Myriad},
title = {{Analysis of the Acceleration of Neural Networks Inference on Intel Processors Based on OpenVINO Toolkit}},
year = {2020}
}
@article{Moloney2014,
author = {Moloney, David and Barry, Co-authors Brendan and Richmond, Richard and Connor, Fergal and Brick, Cormac and Donohoe, David},
file = {:home/tomas/tesis/papers/07478823.pdf:pdf},
journal = {HotChip, slides},
mendeley-groups = {tesis,tesis/Neural Acc Arch},
publisher = {IEEE},
title = {{Myriad 2}},
year = {2014}
}
@article{Cheng2019,
abstract = {This paper proposes to adopt logarithm-approximate multiplier (LAM) for multiply-accumulate (MAC) computation in neural network (NN) training engine, where LAM approximates a floating-point multiplication as an addition resulting in smaller delay, fewer gates, and lower power consumption. Our implementation of NN training engine for a 2-D classification dataset achieves 10{\%} speed-up and 2.5X and 2.3X efficiency improvement in power and area, respectively. LAM is also highly compatible with conventional bit-width scaling (BWS). When BWS is applied with LAM in four test datasets, more than 5.2X power efficiency improvement is achievable with only 1{\%} accuracy degradation, where 2.3X improvement originates from LAM.},
author = {Cheng, Tai Yu and Yu, Jaehoon and Hashimoto, Masanori},
doi = {10.1109/PATMOS.2019.8862162},
file = {:media/tomas/KINGSTON1GB/08862162.pdf:pdf},
isbn = {9781728121031},
journal = {2019 IEEE 29th International Symposium on Power and Timing Modeling, Optimization and Simulation, PATMOS 2019},
keywords = {approximate computing,floating-point unit,logarithmic multiplier,multiply-accumulate operation,neural network,training engine},
mendeley-groups = {tesis/Use Approx Comp for NN},
pages = {91--96},
publisher = {IEEE},
title = {{Minimizing Power for Neural Network Training with Logarithm-Approximate Floating-Point Multiplier}},
year = {2019}
}
@article{Moreau2015,
author = {Moreau, Thierry and Sampson, Adrian},
file = {:home/tomas/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moreau, Sampson - 2015 - Compilation and Hardware Support for Approximate Acceleration.pdf:pdf},
mendeley-groups = {tesis/Use NN for Approx Comp},
title = {{Compilation and Hardware Support for Approximate Acceleration}},
year = {2015}
}
@article{Mittal2016,
abstract = {Approximate computing trades off computation quality with effort expended, and as rising performance demands confront plateauing resource budgets, approximate computing has become not merely attractive, but even imperative. In this article, we present a survey of techniques for approximate computing (AC). We discuss strategies for finding approximable program portions and monitoring output quality, techniques for using AC in different processing units (e.g., CPU, GPU, and FPGA), processor components, memory technologies, and so forth, as well as programming frameworks for AC. We classify these techniques based on several key characteristics to emphasize their similarities and differences. The aim of this article is to provide insights to researchers into working of AC techniques and inspire more efforts in this area to make AC the mainstream computing approach in future systems.},
author = {Mittal, Sparsh},
doi = {10.1145/2893356},
file = {:home/tomas/tesis/papers/document.pdf:pdf},
issn = {15577341},
journal = {ACM Computing Surveys},
keywords = {Approximate computing technique (ACT),Approximate storage,CPU,Classification,FPGA,GPU,Neural networks,Quality configurability,Review},
mendeley-groups = {tesis/Misc},
number = {4},
title = {{A survey of techniques for approximate computing}},
volume = {48},
year = {2016}
}
@article{Han2013,
abstract = {Approximate computing has recently emerged as a promising approach to energy-efficient design of digital systems. Approximate computing relies on the ability of many systems and applications to tolerate some loss of quality or optimality in the computed result. By relaxing the need for fully precise or completely deterministic operations, approximate computing techniques allow substantially improved energy efficiency. This paper reviews recent progress in the area, including design of approximate arithmetic blocks, pertinent error and quality measures, and algorithm-level techniques for approximate computing. {\textcopyright} 2013 IEEE.},
author = {Han, Jie and Orshansky, Michael},
doi = {10.1109/ETS.2013.6569370},
file = {:home/tomas/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Han, Orshansky - 2013 - Approximate computing An emerging paradigm for energy-efficient design.pdf:pdf},
isbn = {9781467363778},
journal = {Proceedings - 2013 18th IEEE European Test Symposium, ETS 2013},
keywords = {adder,approximate computing,low-energy design,multiplier,probabilistic computing,stochastic computation},
mendeley-groups = {tesis/Misc},
pages = {1--6},
publisher = {IEEE},
title = {{Approximate computing: An emerging paradigm for energy-efficient design}},
year = {2013}
}
@article{Xu2016,
abstract = {As one of the most promising energy-efficient computing paradigms, approximate computing has gained a lot of research attention in the past few years. This paper presents a survey of state-of-the-art work in all aspects of approximate computing and highlights future research challenges in this field.},
author = {Xu, Qiang and Mytkowicz, Todd and Kim, Nam Sung},
doi = {10.1109/MDAT.2015.2505723},
file = {:home/tomas/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu, Mytkowicz, Kim - 2016 - Approximate Computing A Survey.pdf:pdf},
issn = {21682356},
journal = {IEEE Design and Test},
keywords = {Approximate computing,Approximation methods,Computational modeling,Computer languages,Probabilistic logic,Runtime},
mendeley-groups = {tesis/Misc},
number = {1},
pages = {8--22},
publisher = {IEEE},
title = {{Approximate Computing: A Survey}},
volume = {33},
year = {2016}
}
@article{Demidovskij2019,
abstract = {A task of maximizing deep learning neural networks performance is a challenging and actual goal of modern hardware and software development. Regardless the huge variety of optimization techniques and emerging dedicated hardware platforms, the process of tuning the performance of the neural network is hard. It requires configuring dozens of hyper parameters of optimization algorithms, selecting appropriate metrics, benchmarking the intermediate solutions to choose the best method, platform etc. Moreover, it is required to setup the hardware for the specific inference target. This paper introduces a sophisticated software solution (Deep Learning Workbench) that provides interactive user interface, simplified process of 8-bit quantization, speeding up convolutional operations using the Winograds minimal filtering algorithms, measuring accuracy of the resulting model. The proposed software is built over the open source OpenVINO framework and supports huge range of modern deep learning models.},
author = {Demidovskij, Alexander and Gorbachev, Yury and Fedorov, Mikhail and Slavutin, Iliya and Tugarev, Artyom and Fatekhov, Marat and Tarkan, Yaroslav},
doi = {10.1109/ICCVW.2019.00104},
file = {:home/tomas/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Demidovskij et al. - 2019 - OpenVINO deep learning workbench Comprehensive analysis and tuning of neural networks inference.pdf:pdf},
isbn = {9781728150239},
journal = {Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019},
keywords = {Deep learning,Inference,Openvino,Optimization},
mendeley-groups = {tesis/Misc},
pages = {783--787},
publisher = {IEEE},
title = {{OpenVINO deep learning workbench: Comprehensive analysis and tuning of neural networks inference}},
year = {2019}
}
@article{Liu2016,
abstract = {Neural Networks (NN) are a family of models for a broadrange of emerging machine learning and pattern reconditionapplications. NN techniques are conventionally executed ongeneral-purpose processors (such as CPU and GPGPU), which areusually not energy-efficient since they invest excessivehardware resources to flexibly support various workloads. Consequently, application-specific hardware accelerators forneural networks have been proposed recently to improve the energy-efficiency. However, such accelerators were designed for a small set of NN techniquessharing similar computational patterns, and they adopt complex and informativeinstructions (control signals) directly corresponding to high-levelfunctional blocks of an NN (such as layers), or even an NN as awhole. Although straightforward and easy-to-implement for a limitedset of similar NN techniques, the lack of agility in the instructionset prevents such accelerator designs from supporting a varietyof different NN techniques with sufficient flexibility and efficiency. In this paper, we propose a novel domain-specific Instruction Set Architecture (ISA) for NNaccelerators, called Cambricon, which is a load-store architecture thatintegrates scalar, vector, matrix, logical, data transfer, and controlinstructions, based on a comprehensive analysis of existing NNtechniques. Our evaluation over a total of ten representative yetdistinct NN techniques have demonstrated that Cambricon exhibits strongdescriptive capacity over a broad range of NN techniques, and provideshigher code density than general-purpose ISAs such as x86, MIPS, andGPGPU. Compared to the latest state-of-the-art NN accelerator design DaDianNao[5] (which can only accommodate 3 types of NN techniques), our Cambricon-based accelerator prototype implemented in TSMC 65nm technologyincurs only negligible latency/power/area overheads, with a versatile coverage of 10 different NN benchmarks.},
author = {Liu, Shaoli and Du, Zidong and Tao, Jinhua and Han, Dong and Luo, Tao and Xie, Yuan and Chen, Yunji and Chen, Tianshi},
doi = {10.1109/ISCA.2016.42},
file = {:home/tomas/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2016 - Cambricon An Instruction Set Architecture for Neural Networks.pdf:pdf},
isbn = {9781467389471},
journal = {Proceedings - 2016 43rd International Symposium on Computer Architecture, ISCA 2016},
mendeley-groups = {tesis/Neural Acc Arch},
pages = {393--405},
publisher = {IEEE},
title = {{Cambricon: An Instruction Set Architecture for Neural Networks}},
year = {2016}
}
@article{Benelli2019,
abstract = {Nowadays Voice User Interfaces (VUIs) have become popular thanks to their easiness of use that makes them accessible to the elderly and people with disability. Nevertheless, their use in embedded systems for the realization of portable devices is limited by the computation complexity, the memory requirements and power consumption of the keyword spotting (KWS) algorithms, usually based on deep neural networks. In this paper we propose a new algorithm based on convolutional neural networks for the keyword spotting task, that offers a good trade-off among accuracy, power consumption and memory footprint. To select our proposed solution, we compared different neural network architectures to select the best trade-off of these metrics. For further improvements of these performances we implemented our solution on a dedicated hardware platform as Myriad 2 by Movidius. The use of this chip has reduced inference time and energy per inference by 50{\%}.},
author = {Benelli, Gionata and Meoni, Gabriele and Fanucci, Luca},
doi = {10.1109/VLSI-SoC.2018.8644728},
file = {:home/tomas/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Benelli, Meoni, Fanucci - 2019 - A low power keyword spotting algorithm for memory constrained embedded systems.pdf:pdf},
isbn = {9781538647561},
issn = {23248440},
journal = {IEEE/IFIP International Conference on VLSI and System-on-Chip, VLSI-SoC},
keywords = {Convolutional neural network,Low-power,Machine learning,Memory footprint,Neural compute stick,Neural network,Speech recognition},
mendeley-groups = {tesis/OpenVINO-Myriad},
pages = {267--272},
publisher = {IEEE},
title = {{A low power keyword spotting algorithm for memory constrained embedded systems}},
volume = {2018-Octob},
year = {2019}
}
@article{HoreaIonica2015,
author = {{Horea Ionica}, Mircea and Gregg, David},
file = {:home/tomas/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Horea Ionica, Gregg - 2015 - The Movidius Myriad Architecture's Potential for Scientific Computing.pdf:pdf},
mendeley-groups = {tesis/Neural Acc Arch},
pages = {6--14},
title = {{The Movidius Myriad Architecture's Potential for Scientific Computing}},
year = {2015}
}
@article{Marantos2018,
abstract = {Support Vector Machines (SVM) classifiers are widely used as inference tools in Internet of Things (IoT) and Edge Computing applications. To achieve high classification accuracy, the SVM classifier can turn out to be the computationally intensive and power hungry component of the application. In this paper, we enable an efficient SVM implementation, in terms of performance and power dissipation, on an ultra-low-power multi-core SoC, Intel/Movidius Myriad 2. Experimental results highlight the efficiency of the proposed solution, as it achieves 105 × speed-up compared to its initial porting and up to 40{\%} energy savings against state-of-the-art relevant approaches.},
author = {Marantos, Charalampos and Karavalakis, Nikolaos and Leon, Vasileios and Tsoutsouras, Vasileios and Pekmestzi, Kiamal and Soudris, Dimitrios},
doi = {10.1109/MOCAST.2018.8376630},
file = {:home/tomas/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Marantos et al. - 2018 - Efficient support vector machines implementation on IntelMovidius Myriad 2.pdf:pdf},
isbn = {9781538647882},
journal = {2018 7th International Conference on Modern Circuits and Systems Technologies, MOCAST 2018},
keywords = {ECG Analysis,Edge Computing,Intel/Movidius Myriad 2,Internet of Things,Support Vector Machines},
mendeley-groups = {tesis/OpenVINO-Myriad},
pages = {1--4},
publisher = {IEEE},
title = {{Efficient support vector machines implementation on Intel/Movidius Myriad 2}},
year = {2018}
}
@article{Khan2018,
abstract = {The advent of exascale computing, with the unparalleled rise in the scale of data in Internet of Things (IoT), high performance computing (HPC), and big data domains, both at the center and the edge of the system, requires optimal exploitation of energy-efficient computing hardware dedicated for edge processing. Emerging hardware for data processing at the edge must take advantage of advanced concurrent data locality-Aware algorithms and data structures in order to provide better throughput and energy efficiency. Their design must be performance portable for their implementation to perform equally well on the edge hardware as well as other high performance computing, embedded and accelerator platforms. Concurrent search trees are one such widely used back-end for many important big data systems, databases, and file systems. We analyze DeltaTree, a concurrent energy-efficient and locality-Aware data structure based on relaxed cache-oblivious model and van Emde Boas trees, on Intel's specialized computing platform Movidius Myriad 2, designed for machine vision and computing capabilities at the edge. We compare the throughput and energy efficiency of DeltaTree with B-link tree, a highly concurrent B+tree, on Movidius Myriad 2, along with a high performance computing platform (Intel Xeon), an ARM embedded platform, and an accelerator platform (Intel Xeon Phi). The results show that DeltaTree is performance portable, providing better energy-efficiency and throughput than B-link tree on these platforms for most workloads. For Movidius Myriad 2 in particular, DeltaTree performs really well with its throughput and efficiency up to 4× better than B-link tree.},
author = {Khan, Amin M. and Umar, Ibrahim and Ha, Phuong Hoai},
doi = {10.1109/HPCS.2018.00060},
file = {:home/tomas/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Khan, Umar, Ha - 2018 - Efficient compute at the edge Optimizing energy aware data structures for emerging edge hardware.pdf:pdf},
isbn = {9781538678787},
journal = {Proceedings - 2018 International Conference on High Performance Computing and Simulation, HPCS 2018},
keywords = {Binary search trees,Concurrent algorithms,Data locality,Edge computing,Energy efficient algorithms},
mendeley-groups = {tesis/OpenVINO-Myriad},
pages = {314--321},
publisher = {IEEE},
title = {{Efficient compute at the edge: Optimizing energy aware data structures for emerging edge hardware}},
year = {2018}
}
@article{Xu2018,
abstract = {2D Convolutional Neural Networks (CNNs) have enjoyed a surge in popularity over the last few years, mainly because they outperform traditional algorithms/methods in a myriad of computer vision (and other fields) tasks. On the other hand, the problem becomes more complex when dealing with 3D volumes. Lack of readily available training data, memory and computational requirements are just some of the factors hindering the progress of 3D CNNs. We propose a synthetic 3D voxelized point-clouds generation method containing object and scene in this paper. Furthermore, an efficient 3D volumetric representation called VOLA is applied. VOLA (Volumetric Accelerator) is a sexaquaternary (power-of-four subdivision) tree-based representation which aims to save significant memory for volumetric data. After training the model, it was deployed onto Movidius Neural Compute Stick which is a USB, containing a low-power processing unit as well as dedicated CNN hardware blocks. The trained model on NCS takes only ∼ 90 frames per second to perform inference on each 3D volume, with an average power consumption of 1.2W.},
author = {Xu, Xiaofan and Amaro, Joao and Caulfield, Sam and Forembski, Andrew and Falcao, Gabriel and Moloney, David},
doi = {10.1109/CISP-BMEI.2017.8302078},
file = {:home/tomas/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu et al. - 2018 - Convolutional neural network on neural compute stick for voxelized point-clouds classification.pdf:pdf},
isbn = {9781538619377},
journal = {Proceedings - 2017 10th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics, CISP-BMEI 2017},
keywords = {Convolutional Neural Networks,Embedded Systems,Point-clouds},
mendeley-groups = {tesis/OpenVINO-Myriad},
pages = {1--7},
title = {{Convolutional neural network on neural compute stick for voxelized point-clouds classification}},
volume = {2018-Janua},
year = {2018}
}
@article{Mathew2019,
abstract = {With the advancement of AI in the field of medical imaging, medical diagnosis is getting faster and viable for medical practitioners especially for cancer diagnosis. Earlier Deep Learning solutions had to be deployed on High Performance Computing devices like GPU for achieving real time performance. But with Optimization on Intel Core and Xeon processors with Intel Distribution of OpenVINO Toolkit (Open Visual Inference and Neural Network Optimization), it is possible to deploy Deep Learning models with accelerated performance, than running Tensorflow / Caffe models on CPU machines. In this paper we describe the proposed work wherein we ported our DetectNet Deep Learning Model with NVIDIA specific custom layer for lung nodule detection trained on LIDC dataset, using Intel Distribution of OpenVINO, and deployed the same in Intel Core/Xeon processors with accelerated performance.},
author = {Mathew, Gina and {Sindhu Ramachandran}, S. and Suchithra, V. S.},
doi = {10.1109/TENCON.2019.8929612},
file = {:home/tomas/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mathew, Sindhu Ramachandran, Suchithra - 2019 - Lung Nodule Detection from low dose CT scan using Optimization on Intel Xeon and Core pr.pdf:pdf},
isbn = {9781728118956},
issn = {21593450},
journal = {IEEE Region 10 Annual International Conference, Proceedings/TENCON},
keywords = {Detect-Net,Intel Distribution of OpenVINO,LIDC},
mendeley-groups = {tesis/OpenVINO-Myriad},
pages = {1783--1788},
publisher = {IEEE},
title = {{Lung Nodule Detection from low dose CT scan using Optimization on Intel Xeon and Core processors with Intel Distribution of OpenVINO Toolkit}},
volume = {2019-Octob},
year = {2019}
}
@article{Tiwari2019,
abstract = {In recent years there has been an extensive development in the field of convolutional neural network-based image classification because of the human-like inference results obtained, but these massive networks are resource intensive and have high memory and computational requirements. Intel's Neural Compute Stick brings real time inference, prototyping and deployment of these DNNs to the network edge. In this paper we will discuss the development of a model for classification of book cover images into genres, and subsequently compiling the trained model for use with the Neural Compute Stick, so as to receive the optimized results in constrained environments thus ultimately leading to a system to judge a book by its cover which can be used even within a low power environment like a mobile device or Raspberry Pi, as the stick runs on power values as low as 1.2W.},
author = {Tiwari, Naman and Mondal, Koushik},
doi = {10.1109/TENSYMP46218.2019.8971238},
file = {:home/tomas/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tiwari, Mondal - 2019 - NCS based ultra low power optimized machine learning techniques for image classification.pdf:pdf},
isbn = {9781728102979},
journal = {Proceedings of 2019 IEEE Region 10 Symposium, TENSYMP 2019},
keywords = {Convolutional Neural Networks,Deep Neural Network,Neural Compute Stick,OpenVINO Toolkit},
mendeley-groups = {tesis/OpenVINO-Myriad},
pages = {750--753},
title = {{NCS based ultra low power optimized machine learning techniques for image classification}},
volume = {7},
year = {2019}
}
@article{Ionica2015,
author={{Horea Ionica}, Mircea and Gregg, David},
journal={IEEE Micro}, 
title={{The Movidius Myriad Architecture's Potential for Scientific Computing}}, 
year={2015},
volume={35},
number={1},
pages={6--14}
}
@misc{openvino_toolkit,
title = {{OpenVINO Toolkit}},
howpublished = {\url{https://software.intel.com/en-us/openvino-toolkit}},
note = {Accessed: 2020-08-19}
}
@misc{ncs2,
title = {{Intel Neural Compute Stick 2}},
howpublished = {\url{https://ark.intel.com/content/www/us/en/ark/products/140109/intel-neural-compute-stick-2.html}},
note = {Accessed: 2020-08-19}
}
@misc{myriadx,
title = {{Myriad X Product Brief}},
howpublished = {\url{https://newsroom.intel.com/wp-content/uploads/sites/11/2017/08/movidius-myriad-xvpu-product-brief.pdf}},
note = {Accessed: 2020-08-19}
}
@misc{myriad_vpus,
title = {{Intel Announces Movidius Myriad X VPU, Featuring ‘Neural Compute Engine’}},
author = {Oh, Nate},
year = {2017-08-28},
howpublished = {\url{https://www.anandtech.com/show/11771/intel-announces-movidius-myriad-x-vpu}},
note = {Accessed: 2020-08-19}
}
@article{Yazdanbakhsh2015,
abstract = {Relaxing the traditional abstraction of 'near-perfect' accuracy in hardware design can lead to significant gains in energy efficiency, area, and performance. To exploit this opportunity, there is a need for design abstractions that can systematically incorporate approximation in hardware design. We introduce Axilog, a set of language annotations, that provides the necessary syntax and semantics for approximate hardware design and reuse in Verilog. Axilog enables the designer to relax the accuracy requirements in certain parts of the design, while keeping the critical parts strictly precise. Axilog is coupled with a Relaxability Inference Analysis that automatically infers the relaxable gates and connections from the designer's annotations. The analysis provides formal safety guarantees that approximation will only affect the parts that the designer intended to approximate, referred to as relaxable elements. Finally, the paper describes a synthesis flow that approximates only the relaxable elements. Axilog enables applying approximation in the synthesis process while abstracting away the details of approximate synthesis from the designer. We evaluate Axilog, its analysis, and the synthesis flow using a diverse set of benchmark designs. The results show that the intuitive nature of the language extensions coupled with the automated analysis enables safe approximation of designs even with thousands of lines of code. Applying our approximate synthesis flow to these designs yields, on average, 54{\%} energy savings and 1.9× area reduction with 10{\%} output quality loss.},
author = {Yazdanbakhsh, Amir and Mahajan, Divya and Thwaites, Bradley and Park, Jongse and Nagendrakumar, Anandhavel and Sethuraman, Sindhuja and Ramkrishnan, Kartik and Ravindran, Nishanthi and Jariwala, Rudra and Rahimi, Abbas and Esmaeilzadeh, Hadi and Bazargan, Kia},
doi = {10.7873/date.2015.0513},
file = {:home/tomas/tesis/papers/misc/2015-date-axilog.pdf:pdf},
isbn = {9783981537048},
issn = {15301591},
journal = {Proceedings -Design, Automation and Test in Europe, DATE},
mendeley-groups = {tesis/Misc},
number = {2},
pages = {812--817},
title = {{Axilog: Language support for approximate hardware design}},
volume = {2015-April},
year = {2015}
}
@misc{fsdd,
title = {{Free Spoken Digit Dataset}},
author = {Jackson, Zohar},
howpublished = {\url{https://github.com/Jakobovski/free-spoken-digit-dataset}},
note = {Accessed: 2020-11-19}
}
@misc{fruit_ds,
title = {{Fruit Images Dataset}},
author = {Oltean, Miha and Muresan, Horea},
howpublished = {\url{https://github.com/Horea94/Fruit-Images-Dataset}},
note = {Accessed: 2020-11-19}
}
@misc{intel_ds,
title = {{Intel Image Classification Dataset}},
author = {Intel},
howpublished = {\url{https://www.kaggle.com/puneet6060/intel-image-classification}},
note = {Accessed: 2020-11-19}
}
@misc{weld,
title = {{Weld Porosity Detection Model}},
author = {Intel},
howpublished = {\url{https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/intel/weld-porosity-detection-0001}},
note = {Accessed: 2020-11-19}
}

@article{Sarwar2018,
abstract = {Deep neural networks (DNNs) have emerged as the state-of-the-art technique in a wide range of machine learning tasks for analytics and computer vision in the next generation of embedded (mobile, IoT, and wearable) devices. Despite their success, they suffer from high energy requirements. In recent years, the inherent error resiliency of DNNs has been exploited by introducing approximations at either the algorithmic or the hardware levels (individually) to obtain energy savings while incurring tolerable accuracy degradation. However, there is a need for investigating the overall energy-accuracy trade-offs arising from the introduction of approximations at different levels in complex DNNs. We perform a comprehensive analysis to determine the effectiveness of cross-layer approximations for the energy-efficient realization of large-scale DNNs. The approximations considered are as follows: 1) use of lower complexity networks (containing lesser number of layers and/or neurons per layer); 2) pruning of synaptic weights; 3) approximate multiplication operation in the neuronal multiply-and-accumulate computation; and 4) approximate write/read operations to/from the synaptic memory. Our experiments on recognition benchmarks (MNIST and CIFAR10) show that cross-layer approximation provides substantial improvements in energy efficiency for different accuracy/quality requirements. Furthermore, we propose a synergistic framework for combining the approximation techniques to achieve maximal energy benefits from approximate DNNs.},
author = {Sarwar, Syed Shakib and Srinivasan, Gopalakrishnan and Han, Bing and Wijesinghe, Parami and Jaiswal, Akhilesh and Panda, Priyadarshini and Raghunathan, Anand and Roy, Kaushik},
doi = {10.1109/JETCAS.2018.2835809},
file = {:home/tomas/tesis/papers/use{\_}approx{\_}comp{\_}for{\_}nn/08358698.pdf:pdf},
issn = {21563357},
journal = {IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
keywords = {Alphabet set multiplier (ASM),Energy efficiency,Hybrid 8T-6T SRAM,Network complexity,Neural computing,Pruning,Shallow network},
mendeley-groups = {tesis,tesis/Use Approx Comp for NN},
number = {4},
pages = {796--809},
publisher = {IEEE},
title = {{Energy efficient neural computing: A study of cross-layer approximations}},
volume = {8},
year = {2018}
}

@article{Hanif2018,
abstract = {Growing interest towards the development of smart Cyber Physical Systems (CPS) and Internet of Things (IoT) has motivated the researchers to explore the suitability of carrying out embedded machine learning. This has enabled a new age of smart CPS and IoT with emerging applications like autonomous vehicles, smart cities and houses, advanced robotics, IoT-Healthcare, and Industry 4.0. Due to the availability of a huge amount of data and compute power, Deep Neural Networks (DNNs) have become one of the enabling technologies behind this current age of machine learning and intelligent systems. The benefits of DNNs however come at a high computational cost and require tremendous amount of energy/power resources that are typically not available on (embedded) IoT and CPS devices, especially when considering the IoT-Edge nodes. To improve the performance and energy/power efficiency of these DNNs, this paper presents a cross-layer approximation methodology which exploits the error resiliency offered by DNNs at various hardware and software layers of the computing stack. We present various case studies at both software and hardware level in order to demonstrate the energy benefits of the proposed methodology. At software level we provide a systematic pruning methodology while at hardware level we provide a case study utilizing approximation of multipliers used for performing the weighted sum operation in the neural processing of DNNs.},
author = {Hanif, Muhammad Abdullah and Marchisio, Alberto and Arif, Tabasher and Hafiz, Rehan and Rehman, Semeen and Shafique, Muhammad},
doi = {10.1166/jolpe.2018.1575},
file = {:home/tomas/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hanif et al. - 2018 - X-DNNs Systematic Cross-Layer Approximations for Energy-Efficient Deep Neural Networks.pdf:pdf},
issn = {15462005},
journal = {Journal of Low Power Electronics},
keywords = {Accelerator,Approximate computing,Approximate multiplier,Artificial intelligence,Co-design,Cross-layer,Deep neural networks,Embedded learning,Energy-efficient design,Error resilience,Hardware,Machine learning,Modeling,Optimization,Pruning,Quantization},
mendeley-groups = {tesis,tesis/Use Approx Comp for NN},
number = {4},
pages = {520--534},
title = {{X-DNNs: Systematic Cross-Layer Approximations for Energy-Efficient Deep Neural Networks}},
volume = {14},
year = {2018}
}

@article{Han2015,
abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9×, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the total number of parameters can be reduced by 13×, from 138 million to 10.3 million, again with no loss of accuracy.},
archivePrefix = {arXiv},
arxivId = {1506.02626},
author = {Han, Song and Pool, Jeff and Tran, John and Dally, William J.},
eprint = {1506.02626},
file = {:home/tomas/tesis/papers/use{\_}approx{\_}comp{\_}for{\_}nn/1506.02626.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {tesis/Use Approx Comp for NN},
pages = {1135--1143},
title = {{Learning both weights and connections for efficient neural networks}},
volume = {2015-January},
year = {2015}
}

@misc{prune1,
title = {{Pruning Deep Neural Networks}},
author = {Singh, Ranjeet},
howpublished = {\url{https://towardsdatascience.com/pruning-deep-neural-network-56cae1ec5505}},
note = {Accessed: 2020-11-19}
}

@misc{quant1,
title = {{How to accelerate and compress neural networks with quantization}},
author = {Danka, Tivadar},
howpublished = {\url{https://towardsdatascience.com/how-to-accelerate-and-compress-neural-networks-with-quantization-edfbbabb6af7}},
note = {Accessed: 2020-11-19}
}

@misc{int8,
title = {{INT8 Calibration}},
author = {Intel},
howpublished = {\url{https://docs.openvinotoolkit.org/latest/workbench_docs_Workbench_DG_Int_8_Quantization.html}},
note = {Accessed: 2020-11-19}
}

@misc{ov_sup_formats,
title = {{OpenVINO Supported Model Formats}},
author = {Intel},
howpublished = {\url{https://docs.openvinotoolkit.org/2020.1/_docs_IE_DG_supported_plugins_Supported_Devices.html#supported_model_formats}},
note = {Accessed: 2020-11-19}
}

@misc{avx,
title = {{AVX Instructions}},
author = {Intel},
howpublished = {\url{https://software.intel.com/content/www/us/en/develop/articles/introduction-to-intel-advanced-vector-extensions.html}},
note = {Accessed: 2020-11-19}
}

@misc{sep_conv,
title = {{A Basic Introduction to Separable Convolutions}},
author = {Wang, Chi-Feng},
howpublished = {\url{https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728}},
note = {Accessed: 2020-11-19}
}

@misc{intro_cnn,
title = {{Introduction to Convolutional Neural Networks}},
author = {Wang, Chi-Feng},
howpublished = {\url{https://towardsdatascience.com/what-is-a-neural-network-6010edabde2b}},
note = {Accessed: 2020-11-19}
}

@misc{guide_cnn,
title = {{A Comprehensive Guide to Convolutional Neural Networks}},
author = {Saha, Sumit},
howpublished = {\url{https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53}},
note = {Accessed: 2020-11-19}
}

@misc{nn_def,
title = {{Types of Neural Networks and Definition of Neural Network}},
author = {Great Learning Team},
howpublished = {\url{https://www.mygreatlearning.com/blog/types-of-neural-networks/}},
note = {Accessed: 2020-11-19}
}

@article{DeMyttenaere2016,
abstract = {We study in this paper the consequences of using the Mean Absolute Percentage Error (MAPE) as a measure of quality for regression models. We prove the existence of an optimal MAPE model and we show the universal consistency of Empirical Risk Minimization based on the MAPE. We also show that finding the best model under the MAPE is equivalent to doing weighted Mean Absolute Error (MAE) regression, and we apply this weighting strategy to kernel regression. The behavior of the MAPE kernel regression is illustrated on simulated data.},
archivePrefix = {arXiv},
arxivId = {1605.02541},
author = {de Myttenaere, Arnaud and Golden, Boris and {Le Grand}, B{\'{e}}n{\'{e}}dicte and Rossi, Fabrice},
doi = {10.1016/j.neucom.2015.12.114},
eprint = {1605.02541},
file = {:home/tomas/tesis/papers/misc/1605.02541.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Consistency,Empirical Risk Minimization,Kernel regression,Mean Absolute Percentage Error,Optimization},
mendeley-groups = {tesis/Misc},
pages = {38--48},
title = {{Mean Absolute Percentage Error for regression models}},
volume = {192},
year = {2016}
}

@misc{wmape,
title = {{WMAPE: Weighted Means Absolute Percentage Error}},
author = {{Institute of Business Forecasting & Planning}},
howpublished = {\url{https://ibf.org/knowledge/glossary/weighted-mean-absolute-percentage-error-wmape-299}},
note = {Accessed: 2020-11-19}
}


@misc{mse,
title = {{Mean Squared Error (MSE)}},
howpublished = {\url{https://www.probabilitycourse.com/chapter9/9_1_5_mean_squared_error_MSE.php}},
note = {Accessed: 2020-11-19}
}
